{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhou\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'utils.util' from 'D:\\\\pycharm_project\\\\optiver-realized-volatility-prediction\\\\src\\\\utils\\\\util.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "from imp import reload\n",
    "import copy as cp\n",
    "from utils import util\n",
    "reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_lst = glob('../data/book_train.parquet/*')\n",
    "stock_lst = [os.path.basename(path).split('=')[-1] for path in path_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "print(len(stock_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isBS B\n",
      "isBS S\n",
      "isBS_big supB\n",
      "isBS_big supS\n",
      "isBS_big midBS\n",
      "isoversize50 up50\n",
      "isoversize50 down50\n",
      "isoversize25 up25\n",
      "isoversize25 down25\n",
      "isoversize75 up75\n",
      "isoversize75 down75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\merge.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  return op.get_result()\n"
     ]
    }
   ],
   "source": [
    "temp = util.gen_data_train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [19:43<00:00, 10.57s/it]\n"
     ]
    }
   ],
   "source": [
    "data_type = 'train'\n",
    "# fe_df, stock_df = util.gen_data_multi(stock_lst, data_type)\n",
    "fe_df = util.gen_data_multi(stock_lst, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_df.to_pickle('../data/train_stock_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fe_df = fe_df.merge(\n",
    "#         stock_df\n",
    "#         , how='left'\n",
    "#         , on='stock_id'\n",
    "#     ).merge(\n",
    "#         train\n",
    "#         , how='left'\n",
    "#         , on=['stock_id', 'time_id']\n",
    "#     ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "\n",
    "fe_df = fe_df.merge(\n",
    "        train\n",
    "        , how='left'\n",
    "        , on=['stock_id', 'time_id']\n",
    "    ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in tqdm(name_lst):\n",
    "#     ret = util.gen_data(name)\n",
    "#     ret.to_csv('../data/20210731/{}.csv'.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  lightgbm  as lgb\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'n_estimators': 10000,\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'seed': 46,\n",
    "    'early_stopping_rounds': 300,\n",
    "    'verbose': -1\n",
    "} \n",
    "from sklearn import model_selection, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSPEMetric(XGBoost=False):\n",
    "    def RMSPE(yhat, dtrain, XGBoost=XGBoost):\n",
    "\n",
    "        y = dtrain.get_label()\n",
    "        elements = ((y - yhat) / y)**2\n",
    "        if XGBoost:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y)))\n",
    "        else:\n",
    "            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n",
    "\n",
    "    return RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = fe_df\n",
    "label = fe_df['target']\n",
    "features = fe_df.columns.difference(['time_id','target']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\zhou\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.7563e-07\ttraining's RMSPE: 0.242859\tvalid_1's l2: 2.81026e-07\tvalid_1's RMSPE: 0.246434\n",
      "[500]\ttraining's l2: 2.49816e-07\ttraining's RMSPE: 0.231207\tvalid_1's l2: 2.57671e-07\tvalid_1's RMSPE: 0.235972\n",
      "[750]\ttraining's l2: 2.41922e-07\ttraining's RMSPE: 0.227524\tvalid_1's l2: 2.54012e-07\tvalid_1's RMSPE: 0.234291\n",
      "[1000]\ttraining's l2: 2.36971e-07\ttraining's RMSPE: 0.225185\tvalid_1's l2: 2.53175e-07\tvalid_1's RMSPE: 0.233904\n",
      "[1250]\ttraining's l2: 2.33245e-07\ttraining's RMSPE: 0.223407\tvalid_1's l2: 2.52427e-07\tvalid_1's RMSPE: 0.233559\n",
      "[1500]\ttraining's l2: 2.30188e-07\ttraining's RMSPE: 0.221938\tvalid_1's l2: 2.52451e-07\tvalid_1's RMSPE: 0.233569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:18, 78.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1345]\ttraining's l2: 2.32017e-07\ttraining's RMSPE: 0.222818\tvalid_1's l2: 2.52254e-07\tvalid_1's RMSPE: 0.233478\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.74743e-07\ttraining's RMSPE: 0.242769\tvalid_1's l2: 2.88874e-07\tvalid_1's RMSPE: 0.247068\n",
      "[500]\ttraining's l2: 2.49331e-07\ttraining's RMSPE: 0.231269\tvalid_1's l2: 2.66252e-07\tvalid_1's RMSPE: 0.237196\n",
      "[750]\ttraining's l2: 2.41532e-07\ttraining's RMSPE: 0.227623\tvalid_1's l2: 2.62081e-07\tvalid_1's RMSPE: 0.235331\n",
      "[1000]\ttraining's l2: 2.36538e-07\ttraining's RMSPE: 0.225258\tvalid_1's l2: 2.60255e-07\tvalid_1's RMSPE: 0.23451\n",
      "[1250]\ttraining's l2: 2.32854e-07\ttraining's RMSPE: 0.223497\tvalid_1's l2: 2.59965e-07\tvalid_1's RMSPE: 0.234379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:30, 74.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1146]\ttraining's l2: 2.34269e-07\ttraining's RMSPE: 0.224175\tvalid_1's l2: 2.59785e-07\tvalid_1's RMSPE: 0.234298\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75232e-07\ttraining's RMSPE: 0.242844\tvalid_1's l2: 2.825e-07\tvalid_1's RMSPE: 0.245615\n",
      "[500]\ttraining's l2: 2.49568e-07\ttraining's RMSPE: 0.231245\tvalid_1's l2: 2.59637e-07\tvalid_1's RMSPE: 0.235467\n",
      "[750]\ttraining's l2: 2.41572e-07\ttraining's RMSPE: 0.227511\tvalid_1's l2: 2.54451e-07\tvalid_1's RMSPE: 0.233103\n",
      "[1000]\ttraining's l2: 2.36592e-07\ttraining's RMSPE: 0.225153\tvalid_1's l2: 2.52132e-07\tvalid_1's RMSPE: 0.232039\n",
      "[1250]\ttraining's l2: 2.32833e-07\ttraining's RMSPE: 0.223357\tvalid_1's l2: 2.51183e-07\tvalid_1's RMSPE: 0.231601\n",
      "[1500]\ttraining's l2: 2.29763e-07\ttraining's RMSPE: 0.22188\tvalid_1's l2: 2.50506e-07\tvalid_1's RMSPE: 0.231289\n",
      "[1750]\ttraining's l2: 2.27134e-07\ttraining's RMSPE: 0.220607\tvalid_1's l2: 2.50193e-07\tvalid_1's RMSPE: 0.231145\n",
      "[2000]\ttraining's l2: 2.24794e-07\ttraining's RMSPE: 0.219468\tvalid_1's l2: 2.49976e-07\tvalid_1's RMSPE: 0.231044\n",
      "[2250]\ttraining's l2: 2.22647e-07\ttraining's RMSPE: 0.218417\tvalid_1's l2: 2.4979e-07\tvalid_1's RMSPE: 0.230958\n",
      "[2500]\ttraining's l2: 2.20605e-07\ttraining's RMSPE: 0.217413\tvalid_1's l2: 2.49585e-07\tvalid_1's RMSPE: 0.230864\n",
      "[2750]\ttraining's l2: 2.18693e-07\ttraining's RMSPE: 0.216469\tvalid_1's l2: 2.49355e-07\tvalid_1's RMSPE: 0.230757\n",
      "[3000]\ttraining's l2: 2.16861e-07\ttraining's RMSPE: 0.215561\tvalid_1's l2: 2.4913e-07\tvalid_1's RMSPE: 0.230653\n",
      "[3250]\ttraining's l2: 2.15132e-07\ttraining's RMSPE: 0.2147\tvalid_1's l2: 2.48978e-07\tvalid_1's RMSPE: 0.230583\n",
      "[3500]\ttraining's l2: 2.13478e-07\ttraining's RMSPE: 0.213872\tvalid_1's l2: 2.49016e-07\tvalid_1's RMSPE: 0.230601\n",
      "Early stopping, best iteration is:\n",
      "[3257]\ttraining's l2: 2.15083e-07\ttraining's RMSPE: 0.214675\tvalid_1's l2: 2.48972e-07\tvalid_1's RMSPE: 0.23058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [04:58, 108.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75197e-07\ttraining's RMSPE: 0.24267\tvalid_1's l2: 2.92914e-07\tvalid_1's RMSPE: 0.251573\n",
      "[500]\ttraining's l2: 2.49633e-07\ttraining's RMSPE: 0.231124\tvalid_1's l2: 2.67873e-07\tvalid_1's RMSPE: 0.24058\n",
      "[750]\ttraining's l2: 2.41632e-07\ttraining's RMSPE: 0.22739\tvalid_1's l2: 2.65067e-07\tvalid_1's RMSPE: 0.239316\n",
      "[1000]\ttraining's l2: 2.36683e-07\ttraining's RMSPE: 0.225049\tvalid_1's l2: 2.67625e-07\tvalid_1's RMSPE: 0.240468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [06:38, 104.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[784]\ttraining's l2: 2.40824e-07\ttraining's RMSPE: 0.22701\tvalid_1's l2: 2.64988e-07\tvalid_1's RMSPE: 0.23928\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.7531e-07\ttraining's RMSPE: 0.242769\tvalid_1's l2: 3.02728e-07\tvalid_1's RMSPE: 0.255287\n",
      "[500]\ttraining's l2: 2.49593e-07\ttraining's RMSPE: 0.231153\tvalid_1's l2: 2.77768e-07\tvalid_1's RMSPE: 0.244536\n",
      "[750]\ttraining's l2: 2.41617e-07\ttraining's RMSPE: 0.227429\tvalid_1's l2: 2.71997e-07\tvalid_1's RMSPE: 0.241983\n",
      "[1000]\ttraining's l2: 2.36639e-07\ttraining's RMSPE: 0.225074\tvalid_1's l2: 2.69367e-07\tvalid_1's RMSPE: 0.24081\n",
      "[1250]\ttraining's l2: 2.32885e-07\ttraining's RMSPE: 0.223282\tvalid_1's l2: 2.67631e-07\tvalid_1's RMSPE: 0.240033\n",
      "[1500]\ttraining's l2: 2.29808e-07\ttraining's RMSPE: 0.221802\tvalid_1's l2: 2.67249e-07\tvalid_1's RMSPE: 0.239861\n",
      "[1750]\ttraining's l2: 2.27143e-07\ttraining's RMSPE: 0.220512\tvalid_1's l2: 2.66743e-07\tvalid_1's RMSPE: 0.239634\n",
      "[2000]\ttraining's l2: 2.24787e-07\ttraining's RMSPE: 0.219366\tvalid_1's l2: 2.66713e-07\tvalid_1's RMSPE: 0.239621\n",
      "[2250]\ttraining's l2: 2.22612e-07\ttraining's RMSPE: 0.218302\tvalid_1's l2: 2.66463e-07\tvalid_1's RMSPE: 0.239508\n",
      "[2500]\ttraining's l2: 2.20604e-07\ttraining's RMSPE: 0.217315\tvalid_1's l2: 2.66418e-07\tvalid_1's RMSPE: 0.239488\n",
      "Early stopping, best iteration is:\n",
      "[2202]\ttraining's l2: 2.23013e-07\ttraining's RMSPE: 0.218498\tvalid_1's l2: 2.66357e-07\tvalid_1's RMSPE: 0.239461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [08:30, 107.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75836e-07\ttraining's RMSPE: 0.242982\tvalid_1's l2: 2.75683e-07\tvalid_1's RMSPE: 0.243789\n",
      "[500]\ttraining's l2: 2.50073e-07\ttraining's RMSPE: 0.231357\tvalid_1's l2: 2.5323e-07\tvalid_1's RMSPE: 0.233651\n",
      "[750]\ttraining's l2: 2.42083e-07\ttraining's RMSPE: 0.227631\tvalid_1's l2: 2.48121e-07\tvalid_1's RMSPE: 0.231282\n",
      "[1000]\ttraining's l2: 2.37068e-07\ttraining's RMSPE: 0.22526\tvalid_1's l2: 2.45601e-07\tvalid_1's RMSPE: 0.230104\n",
      "[1250]\ttraining's l2: 2.33343e-07\ttraining's RMSPE: 0.223484\tvalid_1's l2: 2.44255e-07\tvalid_1's RMSPE: 0.229473\n",
      "[1500]\ttraining's l2: 2.30242e-07\ttraining's RMSPE: 0.221994\tvalid_1's l2: 2.43253e-07\tvalid_1's RMSPE: 0.229002\n",
      "[1750]\ttraining's l2: 2.27607e-07\ttraining's RMSPE: 0.22072\tvalid_1's l2: 2.42727e-07\tvalid_1's RMSPE: 0.228754\n",
      "[2000]\ttraining's l2: 2.25211e-07\ttraining's RMSPE: 0.219555\tvalid_1's l2: 2.42234e-07\tvalid_1's RMSPE: 0.228522\n",
      "[2250]\ttraining's l2: 2.23073e-07\ttraining's RMSPE: 0.218511\tvalid_1's l2: 2.41831e-07\tvalid_1's RMSPE: 0.228332\n",
      "[2500]\ttraining's l2: 2.21055e-07\ttraining's RMSPE: 0.21752\tvalid_1's l2: 2.41522e-07\tvalid_1's RMSPE: 0.228186\n",
      "[2750]\ttraining's l2: 2.19154e-07\ttraining's RMSPE: 0.216582\tvalid_1's l2: 2.41308e-07\tvalid_1's RMSPE: 0.228084\n",
      "[3000]\ttraining's l2: 2.17337e-07\ttraining's RMSPE: 0.215683\tvalid_1's l2: 2.41172e-07\tvalid_1's RMSPE: 0.22802\n",
      "[3250]\ttraining's l2: 2.1563e-07\ttraining's RMSPE: 0.214834\tvalid_1's l2: 2.4106e-07\tvalid_1's RMSPE: 0.227967\n",
      "[3500]\ttraining's l2: 2.1397e-07\ttraining's RMSPE: 0.214006\tvalid_1's l2: 2.40847e-07\tvalid_1's RMSPE: 0.227866\n",
      "[3750]\ttraining's l2: 2.12375e-07\ttraining's RMSPE: 0.213207\tvalid_1's l2: 2.40747e-07\tvalid_1's RMSPE: 0.227819\n",
      "[4000]\ttraining's l2: 2.10819e-07\ttraining's RMSPE: 0.212424\tvalid_1's l2: 2.40623e-07\tvalid_1's RMSPE: 0.22776\n",
      "[4250]\ttraining's l2: 2.0929e-07\ttraining's RMSPE: 0.211652\tvalid_1's l2: 2.40543e-07\tvalid_1's RMSPE: 0.227722\n",
      "[4500]\ttraining's l2: 2.07851e-07\ttraining's RMSPE: 0.210923\tvalid_1's l2: 2.40476e-07\tvalid_1's RMSPE: 0.227691\n",
      "[4750]\ttraining's l2: 2.0644e-07\ttraining's RMSPE: 0.210206\tvalid_1's l2: 2.40398e-07\tvalid_1's RMSPE: 0.227654\n",
      "[5000]\ttraining's l2: 2.05047e-07\ttraining's RMSPE: 0.209496\tvalid_1's l2: 2.4034e-07\tvalid_1's RMSPE: 0.227626\n",
      "[5250]\ttraining's l2: 2.03701e-07\ttraining's RMSPE: 0.208807\tvalid_1's l2: 2.40299e-07\tvalid_1's RMSPE: 0.227607\n",
      "Early stopping, best iteration is:\n",
      "[5164]\ttraining's l2: 2.04169e-07\ttraining's RMSPE: 0.209047\tvalid_1's l2: 2.4029e-07\tvalid_1's RMSPE: 0.227603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [12:13, 146.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75378e-07\ttraining's RMSPE: 0.242962\tvalid_1's l2: 2.80987e-07\tvalid_1's RMSPE: 0.244471\n",
      "[500]\ttraining's l2: 2.49431e-07\ttraining's RMSPE: 0.231232\tvalid_1's l2: 2.58628e-07\tvalid_1's RMSPE: 0.234543\n",
      "[750]\ttraining's l2: 2.41555e-07\ttraining's RMSPE: 0.227552\tvalid_1's l2: 2.5385e-07\tvalid_1's RMSPE: 0.232366\n",
      "[1000]\ttraining's l2: 2.36595e-07\ttraining's RMSPE: 0.225204\tvalid_1's l2: 2.51637e-07\tvalid_1's RMSPE: 0.231351\n",
      "[1250]\ttraining's l2: 2.32904e-07\ttraining's RMSPE: 0.22344\tvalid_1's l2: 2.50523e-07\tvalid_1's RMSPE: 0.230838\n",
      "[1500]\ttraining's l2: 2.29839e-07\ttraining's RMSPE: 0.221966\tvalid_1's l2: 2.49769e-07\tvalid_1's RMSPE: 0.230491\n",
      "[1750]\ttraining's l2: 2.27255e-07\ttraining's RMSPE: 0.220714\tvalid_1's l2: 2.4944e-07\tvalid_1's RMSPE: 0.230339\n",
      "[2000]\ttraining's l2: 2.24878e-07\ttraining's RMSPE: 0.219557\tvalid_1's l2: 2.49061e-07\tvalid_1's RMSPE: 0.230164\n",
      "[2250]\ttraining's l2: 2.22679e-07\ttraining's RMSPE: 0.218481\tvalid_1's l2: 2.48741e-07\tvalid_1's RMSPE: 0.230016\n",
      "[2500]\ttraining's l2: 2.20647e-07\ttraining's RMSPE: 0.217481\tvalid_1's l2: 2.48533e-07\tvalid_1's RMSPE: 0.229919\n",
      "[2750]\ttraining's l2: 2.18749e-07\ttraining's RMSPE: 0.216544\tvalid_1's l2: 2.48488e-07\tvalid_1's RMSPE: 0.229899\n",
      "[3000]\ttraining's l2: 2.16928e-07\ttraining's RMSPE: 0.215641\tvalid_1's l2: 2.48464e-07\tvalid_1's RMSPE: 0.229888\n",
      "[3250]\ttraining's l2: 2.15175e-07\ttraining's RMSPE: 0.214768\tvalid_1's l2: 2.48398e-07\tvalid_1's RMSPE: 0.229857\n",
      "[3500]\ttraining's l2: 2.13506e-07\ttraining's RMSPE: 0.213933\tvalid_1's l2: 2.48242e-07\tvalid_1's RMSPE: 0.229785\n",
      "[3750]\ttraining's l2: 2.11927e-07\ttraining's RMSPE: 0.213141\tvalid_1's l2: 2.482e-07\tvalid_1's RMSPE: 0.229766\n",
      "[4000]\ttraining's l2: 2.10393e-07\ttraining's RMSPE: 0.212368\tvalid_1's l2: 2.48222e-07\tvalid_1's RMSPE: 0.229776\n",
      "Early stopping, best iteration is:\n",
      "[3859]\ttraining's l2: 2.1125e-07\ttraining's RMSPE: 0.2128\tvalid_1's l2: 2.4816e-07\tvalid_1's RMSPE: 0.229747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [15:28, 162.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75318e-07\ttraining's RMSPE: 0.24294\tvalid_1's l2: 2.80345e-07\tvalid_1's RMSPE: 0.244153\n",
      "[500]\ttraining's l2: 2.49462e-07\ttraining's RMSPE: 0.231251\tvalid_1's l2: 2.58374e-07\tvalid_1's RMSPE: 0.23439\n",
      "[750]\ttraining's l2: 2.41633e-07\ttraining's RMSPE: 0.227593\tvalid_1's l2: 2.53871e-07\tvalid_1's RMSPE: 0.232339\n",
      "[1000]\ttraining's l2: 2.36599e-07\ttraining's RMSPE: 0.22521\tvalid_1's l2: 2.51856e-07\tvalid_1's RMSPE: 0.231415\n",
      "[1250]\ttraining's l2: 2.32889e-07\ttraining's RMSPE: 0.223437\tvalid_1's l2: 2.50971e-07\tvalid_1's RMSPE: 0.231008\n",
      "[1500]\ttraining's l2: 2.29856e-07\ttraining's RMSPE: 0.221978\tvalid_1's l2: 2.50446e-07\tvalid_1's RMSPE: 0.230766\n",
      "[1750]\ttraining's l2: 2.27227e-07\ttraining's RMSPE: 0.220704\tvalid_1's l2: 2.50367e-07\tvalid_1's RMSPE: 0.230729\n",
      "[2000]\ttraining's l2: 2.24848e-07\ttraining's RMSPE: 0.219546\tvalid_1's l2: 2.50222e-07\tvalid_1's RMSPE: 0.230663\n",
      "[2250]\ttraining's l2: 2.22692e-07\ttraining's RMSPE: 0.218491\tvalid_1's l2: 2.50045e-07\tvalid_1's RMSPE: 0.230581\n",
      "[2500]\ttraining's l2: 2.20664e-07\ttraining's RMSPE: 0.217494\tvalid_1's l2: 2.49776e-07\tvalid_1's RMSPE: 0.230457\n",
      "[2750]\ttraining's l2: 2.18749e-07\ttraining's RMSPE: 0.216548\tvalid_1's l2: 2.49854e-07\tvalid_1's RMSPE: 0.230493\n",
      "Early stopping, best iteration is:\n",
      "[2544]\ttraining's l2: 2.20321e-07\ttraining's RMSPE: 0.217325\tvalid_1's l2: 2.4973e-07\tvalid_1's RMSPE: 0.230436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [17:35, 151.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75855e-07\ttraining's RMSPE: 0.242944\tvalid_1's l2: 2.79835e-07\tvalid_1's RMSPE: 0.246035\n",
      "[500]\ttraining's l2: 2.49989e-07\ttraining's RMSPE: 0.231274\tvalid_1's l2: 2.5627e-07\tvalid_1's RMSPE: 0.235448\n",
      "[750]\ttraining's l2: 2.4202e-07\ttraining's RMSPE: 0.227558\tvalid_1's l2: 2.52812e-07\tvalid_1's RMSPE: 0.233854\n",
      "[1000]\ttraining's l2: 2.3708e-07\ttraining's RMSPE: 0.225223\tvalid_1's l2: 2.52109e-07\tvalid_1's RMSPE: 0.233529\n",
      "[1250]\ttraining's l2: 2.33397e-07\ttraining's RMSPE: 0.223467\tvalid_1's l2: 2.5229e-07\tvalid_1's RMSPE: 0.233612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [18:50, 127.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1173]\ttraining's l2: 2.34446e-07\ttraining's RMSPE: 0.223969\tvalid_1's l2: 2.51736e-07\tvalid_1's RMSPE: 0.233356\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[250]\ttraining's l2: 2.75059e-07\ttraining's RMSPE: 0.242839\tvalid_1's l2: 2.86155e-07\tvalid_1's RMSPE: 0.246543\n",
      "[500]\ttraining's l2: 2.4927e-07\ttraining's RMSPE: 0.231175\tvalid_1's l2: 2.61923e-07\tvalid_1's RMSPE: 0.235873\n",
      "[750]\ttraining's l2: 2.4152e-07\ttraining's RMSPE: 0.227553\tvalid_1's l2: 2.57349e-07\tvalid_1's RMSPE: 0.233805\n",
      "[1000]\ttraining's l2: 2.36568e-07\ttraining's RMSPE: 0.225208\tvalid_1's l2: 2.54799e-07\tvalid_1's RMSPE: 0.232643\n",
      "[1250]\ttraining's l2: 2.32835e-07\ttraining's RMSPE: 0.223424\tvalid_1's l2: 2.5366e-07\tvalid_1's RMSPE: 0.232123\n",
      "[1500]\ttraining's l2: 2.29777e-07\ttraining's RMSPE: 0.221952\tvalid_1's l2: 2.52929e-07\tvalid_1's RMSPE: 0.231788\n",
      "[1750]\ttraining's l2: 2.27167e-07\ttraining's RMSPE: 0.220688\tvalid_1's l2: 2.52385e-07\tvalid_1's RMSPE: 0.231539\n",
      "[2000]\ttraining's l2: 2.24757e-07\ttraining's RMSPE: 0.219514\tvalid_1's l2: 2.52033e-07\tvalid_1's RMSPE: 0.231377\n",
      "[2250]\ttraining's l2: 2.22592e-07\ttraining's RMSPE: 0.218454\tvalid_1's l2: 2.51741e-07\tvalid_1's RMSPE: 0.231243\n",
      "[2500]\ttraining's l2: 2.20584e-07\ttraining's RMSPE: 0.217467\tvalid_1's l2: 2.51588e-07\tvalid_1's RMSPE: 0.231173\n",
      "[2750]\ttraining's l2: 2.18688e-07\ttraining's RMSPE: 0.21653\tvalid_1's l2: 2.514e-07\tvalid_1's RMSPE: 0.231086\n",
      "[3000]\ttraining's l2: 2.16866e-07\ttraining's RMSPE: 0.215626\tvalid_1's l2: 2.51277e-07\tvalid_1's RMSPE: 0.23103\n",
      "[3250]\ttraining's l2: 2.15154e-07\ttraining's RMSPE: 0.214774\tvalid_1's l2: 2.5129e-07\tvalid_1's RMSPE: 0.231036\n",
      "[3500]\ttraining's l2: 2.1351e-07\ttraining's RMSPE: 0.213951\tvalid_1's l2: 2.51179e-07\tvalid_1's RMSPE: 0.230985\n",
      "[3750]\ttraining's l2: 2.11918e-07\ttraining's RMSPE: 0.213152\tvalid_1's l2: 2.51384e-07\tvalid_1's RMSPE: 0.231079\n",
      "Early stopping, best iteration is:\n",
      "[3588]\ttraining's l2: 2.1294e-07\ttraining's RMSPE: 0.213666\tvalid_1's l2: 2.51102e-07\tvalid_1's RMSPE: 0.230949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [21:32, 129.29s/it]\n"
     ]
    }
   ],
   "source": [
    "cats = ['stock_id', ]\n",
    "# X_train = data_.reset_index(drop=True)\n",
    "X_train = fe_df[features].reset_index(drop=True)\n",
    "\n",
    "y_train = label\n",
    "# y_train = pd.DataFrame(label_)\n",
    "models = []\n",
    "# oof_df = X_train[['time_id', 'stock_id']].copy()\n",
    "oof_df = fe_df[['time_id', 'stock_id']].copy()\n",
    "\n",
    "oof_df['target'] = y_train\n",
    "oof_df['pred'] = np.nan\n",
    "\n",
    "cv = model_selection.KFold(n_splits=10,\n",
    "                            shuffle=True,\n",
    "                            random_state=666)\n",
    "\n",
    "kf = cv.split(X_train, y_train)\n",
    "\n",
    "fi_df = pd.DataFrame()\n",
    "fi_df['features'] = features\n",
    "fi_df['importance'] = 0\n",
    "\n",
    "for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n",
    "    # split\n",
    "    X_tr = X_train.loc[train_index, features]\n",
    "    X_val = X_train.loc[valid_index, features]\n",
    "    y_tr = y_train.loc[train_index].values.reshape(-1)\n",
    "    y_val = y_train.loc[valid_index].values.reshape(-1)\n",
    "\n",
    "    # model (note inverse weighting)\n",
    "    train_set = lgb.Dataset(X_tr,\n",
    "                            y_tr,\n",
    "                            categorical_feature=cats,\n",
    "                            weight=1 / np.power(y_tr, 2))\n",
    "    val_set = lgb.Dataset(X_val,\n",
    "                          y_val,\n",
    "                          categorical_feature=cats,\n",
    "                          weight=1 / np.power(y_val, 2))\n",
    "    model = lgb.train(params,\n",
    "                      train_set,\n",
    "                      valid_sets=[train_set, val_set],\n",
    "                      feval=RMSPEMetric(),\n",
    "                      verbose_eval=250)\n",
    "\n",
    "    # feature importance\n",
    "    fi_df[f'importance_fold{fold_id}'] = model.feature_importance(\n",
    "        importance_type=\"gain\")\n",
    "    fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n",
    "\n",
    "    # save model\n",
    "    joblib.dump(model, f'model_fold{fold_id}.pkl')\n",
    "    logger.debug('model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rollingstats(rolling_x, roll_name):\n",
    "    #统计量\n",
    "    roll_autocorr =  rolling_x.groupby(\"time_id\")[[roll_name,\"xpre\"]].corr()\n",
    "    roll_autocorr .reset_index(inplace=True)\n",
    "    roll_autocorr = roll_autocorr.groupby(\"time_id\").head(1)\n",
    "    roll_autocorr.index = roll_autocorr[\"time_id\"]\n",
    "    del roll_autocorr[\"time_id\"]\n",
    "    roll_autocorr = pd.DataFrame({roll_name+\"_autocorr\": roll_autocorr[\"xpre\"]})\n",
    "    \n",
    "    roll_mean = pd.DataFrame({roll_name+\"_mean\": rolling_x.groupby(\"time_id\")[roll_name].mean()})\n",
    "    roll_std = pd.DataFrame({roll_name+\"_std\": rolling_x.groupby(\"time_id\")[roll_name].std()})\n",
    "    roll_skew = pd.DataFrame({roll_name+\"_skew\": rolling_x.groupby(\"time_id\")[roll_name].skew()})\n",
    "    \n",
    "    data_merge = pd.merge(roll_mean, roll_std, left_index=True, right_index=True,how=\"inner\")\n",
    "    data_merge = pd.merge(data_merge , roll_skew, left_index=True, right_index=True,how=\"inner\")\n",
    "    data_merge = pd.merge(data_merge , roll_autocorr, left_index=True, right_index=True,how=\"inner\")\n",
    "    return data_merge\n",
    "\n",
    "def make_candle(df_data, price_name, vol_name, amt_name):\n",
    "    \n",
    "    df_data[\"pre\"] = df_data.groupby(\"time_id\")[price_name].shift(1)\n",
    "    df_data[\"ret\"] = df_data[price_name] / df_data[\"pre\"] - 1\n",
    "    df_data[\"absret\"] = abs(df_data[\"ret\"] )\n",
    "    df_retsum = pd.DataFrame({\"retsum\":df_data.groupby(\"time_id\")[\"ret\"].sum()})\n",
    "    df_absretsum = pd.DataFrame({\"absretsum\":df_data.groupby(\"time_id\")[\"absret\"].sum()})\n",
    "    \n",
    "    df_data[\"absobv\"] = df_data[\"absret\"] * df_data[vol_name]\n",
    "    df_obvabs = pd.DataFrame({\"xf4_abs\":df_data.groupby(\"time_id\")[\"absobv\"].sum()})\n",
    "    \n",
    "    df_data[\"obv\"] = df_data[\"ret\"] * df_data[vol_name]\n",
    "    df_obv = pd.DataFrame({\"xf4\":df_data.groupby(\"time_id\")[\"obv\"].sum()})\n",
    "    \n",
    "    df_amt =  pd.DataFrame({amt_name + \"sum\": df_data.groupby(\"time_id\")[amt_name].sum()})\n",
    "    df_vol =  pd.DataFrame({vol_name + \"sum\": df_data.groupby(\"time_id\")[vol_name].sum()})\n",
    "    \n",
    "    df_mean = pd.DataFrame({price_name + \"mean\": df_data.groupby(\"time_id\")[price_name].mean()})\n",
    "    df_high = pd.DataFrame({price_name + \"high\": df_data.groupby(\"time_id\")[price_name].max()})\n",
    "    df_low = pd.DataFrame({price_name + \"low\": df_data.groupby(\"time_id\")[price_name].min()})\n",
    "    \n",
    "    df_open = df_data.groupby(\"time_id\").head(1)\n",
    "    df_open.index = df_open [\"time_id\"]\n",
    "    df_open = pd.DataFrame({price_name + \"open\": df_open[price_name]})\n",
    "    \n",
    "    df_close = df_data.groupby(\"time_id\").tail(1)\n",
    "    df_close.index = df_close[\"time_id\"]\n",
    "    df_close = pd.DataFrame({price_name + \"close\": df_close[price_name]})\n",
    "    \n",
    "    df_candle = pd.merge(df_high, df_low, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_mean, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_open, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_close, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_vol, left_index=True, right_index=True,how=\"inner\")\n",
    "    df_candle = pd.merge(df_candle, df_amt, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_retsum, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_absretsum, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_obvabs, left_index=True, right_index=True,how=\"inner\")    \n",
    "    df_candle = pd.merge(df_candle, df_obv, left_index=True, right_index=True,how=\"inner\")    \n",
    "    \n",
    "    return df_candle \n",
    "\n",
    "def cal_candlefactor(df_candle, price_name, vol_name, amt_name):\n",
    "    f_name = price_name + \"candle\"\n",
    "    #f1:illiq\n",
    "    df_candle[f_name + \"f1\"] = (2 * (df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \n",
    "                    - abs(df_candle[price_name + \"open\"] - df_candle[price_name + \"close\"]))/df_candle[amt_name + \"sum\"]\n",
    "    #f2 strength\n",
    "    df_candle[f_name + \"f2\"] = df_candle[\"retsum\"]/df_candle[\"absretsum\"]\n",
    "    #f3:ad\n",
    "    df_candle[f_name + \"f3\"] =  (2 *df_candle[price_name + \"close\"] - df_candle[price_name + \"low\"]\\\n",
    "                    - df_candle[price_name + \"high\"] )/(df_candle[price_name + \"high\"] - df_candle[price_name + \"low\"]) \\\n",
    "                    * df_candle[vol_name + \"sum\"]\n",
    "    #f3: obv\n",
    "    df_candle[f_name + \"f41\"] =  df_candle[\"xf4\"]/df_candle[vol_name + \"sum\"]\n",
    "    df_candle[f_name + \"f42\"] =  df_candle[\"xf4_abs\"]/df_candle[vol_name + \"sum\"]\n",
    "    return df_candle\n",
    "\n",
    "# def calc_rollingstats_realvol(rolling_x, roll_name):\n",
    "#     #统计量\n",
    "#     vol_new =  pd.DataFrame({roll_name+\"_autocorr\": rolling_x.groupby('time_id')[roll_name].apply(\n",
    "#                                     lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))})\n",
    "#     data_merge = pd.merge(data_merge , vol_new, left_index=True, right_index=True,how=\"inner\")\n",
    "#     return data_merge\n",
    "\n",
    "def calculate_features2(book_df, trade_df):\n",
    "    \"\"\"\n",
    "    df: book_train data for each stock_id\n",
    "    \"\"\"\n",
    "    #calculate price for features\n",
    "    \n",
    "    book_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * \n",
    "                       book_df['bid_size1']) / (book_df['bid_size1']+ book_df['ask_size1'])\n",
    "    \n",
    "    book_df[\"vol_ab\"] = book_df['bid_size1']+ book_df['ask_size1']\n",
    "    book_df[\"amt_ab\"] = book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * book_df['bid_size1']\n",
    "    \n",
    "    book_df[\"amt_a\"] = book_df['ask_price1'] * book_df['ask_size1'] \n",
    "    book_df[\"amt_b\"] = book_df['bid_price1'] * book_df['bid_size1'] \n",
    "    \n",
    "    trade_df[\"amt\"] = trade_df[\"price\"] * trade_df[\"size\"]\n",
    "    \n",
    "    #flag filter\n",
    "    book_df[\"wap_pre\"] = book_df.groupby(\"time_id\")['wap'].shift(1)\n",
    "    book_df[\"bid_ppre\"] = book_df.groupby(\"time_id\")['bid_price1'].shift(1)\n",
    "    book_df[\"ask_ppre\"] = book_df.groupby(\"time_id\")['ask_price1'].shift(1)\n",
    "    \n",
    "    \n",
    "    book_df[\"isBS\"] = np.where(book_df[\"wap\"]>book_df[\"wap_pre\"], \"B\",\n",
    "                       np.where(book_df[\"wap\"]<book_df[\"wap_pre\"], \"S\",np.nan))\n",
    "    book_df[\"isBS_big\"] = np.where(book_df[\"wap\"]>book_df[\"ask_ppre\"], \"supB\",\n",
    "                       np.where(book_df[\"wap\"]<book_df[\"bid_ppre\"], \"supS\",\n",
    "                        np.where(pd.notnull(book_df[\"wap\"]),\"midBS\",np.nan)))\n",
    "    \n",
    "    ordersize50 = pd.DataFrame({\"ordersize50\":book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanmedian(x))})\n",
    "    ordersize50.reset_index(inplace=True)\n",
    "    \n",
    "    ordersize25 = pd.DataFrame({\"ordersize25\": book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanpercentile(x,75))})\n",
    "    ordersize25.reset_index(inplace=True)\n",
    "    \n",
    "    ordersize75 = pd.DataFrame({\"ordersize75\": book_df.groupby(\"time_id\")[\"amt_ab\"].apply(lambda x :np.nanpercentile(x, 25))}) \n",
    "    ordersize75.reset_index(inplace=True)\n",
    "    book_df1 = pd.merge(book_df, ordersize50, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize25, on=\"time_id\", how=\"left\")\n",
    "    book_df1 = pd.merge(book_df1, ordersize75, on=\"time_id\", how=\"left\")\n",
    "    book_df1.loc[:,\"isoversize50\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize50\"],\"up50\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize50\"],\"down50\",np.nan))\n",
    "    \n",
    "    \n",
    "    book_df1.loc[:,\"isoversize75\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize75\"],\"up75\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize75\"],\"down75\",np.nan))\n",
    "    book_df1.loc[:,\"isoversize25\"] = np.where(book_df1[\"amt_ab\"]>book_df1[\"ordersize25\"],\"up25\",\n",
    "                                     np.where(book_df1[\"amt_ab\"]<=book_df1[\"ordersize25\"],\"down25\",np.nan))\n",
    "    \n",
    "     #不同波动率\n",
    "    #calculate historical volatility\n",
    "    vol = book_df1.groupby('time_id')['wap'].apply(lambda x: np.sqrt(np.sum(np.log(x).diff()**2)))\n",
    "    vol_df = pd.DataFrame(vol)\n",
    "    vol_df.rename(columns={'wap': 'vol_orig'}, inplace=True)\n",
    "    data_merge_all = vol_df\n",
    "        \n",
    "    #修改波动率：\n",
    "#    rolling波动率均值，标准差，偏度，自相关\n",
    "    #新指标,新指标均值，标准差，偏度，自相关\n",
    "\n",
    "#    roll_name0 = \"roll_std\" \n",
    "#    roll_window = 10\n",
    "    #BS FLAG\n",
    "    flagname = \"B\"\n",
    "    filtername = \"isBS\"\n",
    "    for  filtername, flagname in [[\"isBS\",\"B\"],[\"isBS\",\"S\"],\n",
    "                                  [\"isBS_big\",\"supB\"],[\"isBS_big\",\"supS\"],[\"isBS_big\",\"midBS\"],\n",
    "                                  [\"isoversize50\",\"up50\"],[\"isoversize50\",\"down50\"],\n",
    "                                  [\"isoversize25\",\"up25\"],[\"isoversize25\",\"down25\"],\n",
    "                                  [\"isoversize75\",\"up75\"],[\"isoversize75\",\"down75\"]]:\n",
    "        print(filtername, flagname)\n",
    "        book_df_new = book_df1[book_df1[filtername] == flagname]\n",
    "         #个数\n",
    "        df_fnum = pd.DataFrame({flagname + \"num\":book_df_new .groupby(\"time_id\")[\"seconds_in_bucket\"].count()})\n",
    "        data_merge_all =  pd.merge(data_merge_all, df_fnum, left_index=True, right_index=True,how=\"left\")\n",
    "    \n",
    "    \n",
    "        for roll_window in [5, 10]:\n",
    "            #rolling指标\n",
    "            price_name = \"wap\"\n",
    "            roll_name0 = price_name + \"roll_std\" \n",
    "            roll_name = roll_name0 + str(roll_window)+\"_\"+flagname\n",
    "            \n",
    "            rolling_x = pd.DataFrame({roll_name:book_df_new.groupby(\"time_id\")[price_name].rolling(roll_window).std()})\n",
    "            rolling_x.reset_index(inplace=True)\n",
    "            rolling_x.loc[:,\"xpre\"] = rolling_x.groupby(\"time_id\")[roll_name].shift(1)\n",
    "            #计算统计量因子\n",
    "            data_merge = calc_rollingstats(rolling_x, roll_name)\n",
    "            data_merge_all =  pd.merge(data_merge_all, data_merge, left_index=True, right_index=True,how=\"left\")\n",
    "        \n",
    "        \n",
    "        #全局做candle：wap，买盘，卖盘\n",
    "        #candle因子\n",
    "        price_name = \"wap\"\n",
    "        vol_name = \"vol_ab\"\n",
    "        amt_name = \"amt_ab\"\n",
    "        df_data = cp.deepcopy(book_df_new)\n",
    "        df_candle  = make_candle(df_data, price_name, vol_name, amt_name)\n",
    "        \n",
    "        list_save = [price_name + \"candlef1\", price_name + \"candlef2\", price_name + \"candlef3\",\n",
    "                     price_name + \"candlef41\", price_name + \"candlef42\"]\n",
    "       \n",
    "        list_save = [i +\"_\"+flagname for i in list_save ]\n",
    "        df_candle = cal_candlefactor(df_candle, price_name, vol_name, amt_name)\n",
    "        \n",
    "        col_orig = list(df_candle.columns)\n",
    "        col_new = [i +\"_\"+flagname for i in col_orig ]\n",
    "        df_candle.columns = col_new\n",
    "        \n",
    "        data_merge_all = pd.merge(data_merge_all, df_candle[list_save],left_index=True, right_index=True,how=\"left\")\n",
    "\n",
    "    #加filter做candle\n",
    "    \n",
    "#    切割，打flag，给权重， 加filter切历史，波动率，分买入卖出，大单小单，上行下行，主买主卖\n",
    "#    全天上行波动率/全天波动率    \n",
    "    #index 数据   #复杂    \n",
    "    #calculate max and min bid-ask spread\n",
    "    del data_merge_all[\"vol_orig\"]\n",
    "    \n",
    "    return data_merge_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "book0 = util.load_book(0)\n",
    "trade0 = util.load_trade(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isBS B\n",
      "isBS S\n",
      "isBS_big supB\n",
      "isBS_big supS\n",
      "isBS_big midBS\n",
      "isoversize50 up50\n",
      "isoversize50 down50\n",
      "isoversize25 up25\n",
      "isoversize25 down25\n",
      "isoversize75 up75\n",
      "isoversize75 down75\n"
     ]
    }
   ],
   "source": [
    "temp = calculate_features2(book0,trade0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_id\n",
      "Bnum\n",
      "waproll_std5_B_mean\n",
      "waproll_std5_B_std\n",
      "waproll_std5_B_skew\n",
      "waproll_std5_B_autocorr\n",
      "waproll_std10_B_mean\n",
      "waproll_std10_B_std\n",
      "waproll_std10_B_skew\n",
      "waproll_std10_B_autocorr\n",
      "wapcandlef1_B\n",
      "wapcandlef2_B\n",
      "wapcandlef3_B\n",
      "wapcandlef41_B\n",
      "wapcandlef42_B\n",
      "Snum\n",
      "waproll_std5_S_mean\n",
      "waproll_std5_S_std\n",
      "waproll_std5_S_skew\n",
      "waproll_std5_S_autocorr\n",
      "waproll_std10_S_mean\n",
      "waproll_std10_S_std\n",
      "waproll_std10_S_skew\n",
      "waproll_std10_S_autocorr\n",
      "wapcandlef1_S\n",
      "wapcandlef2_S\n",
      "wapcandlef3_S\n",
      "wapcandlef41_S\n",
      "wapcandlef42_S\n",
      "supBnum\n",
      "waproll_std5_supB_mean\n",
      "waproll_std5_supB_std\n",
      "waproll_std5_supB_skew\n",
      "waproll_std5_supB_autocorr\n",
      "waproll_std10_supB_mean\n",
      "waproll_std10_supB_std\n",
      "waproll_std10_supB_skew\n",
      "waproll_std10_supB_autocorr\n",
      "wapcandlef1_supB\n",
      "wapcandlef2_supB\n",
      "wapcandlef3_supB\n",
      "wapcandlef41_supB\n",
      "wapcandlef42_supB\n",
      "supSnum\n",
      "waproll_std5_supS_mean\n",
      "waproll_std5_supS_std\n",
      "waproll_std5_supS_skew\n",
      "waproll_std5_supS_autocorr\n",
      "waproll_std10_supS_mean\n",
      "waproll_std10_supS_std\n",
      "waproll_std10_supS_skew\n",
      "waproll_std10_supS_autocorr\n",
      "wapcandlef1_supS\n",
      "wapcandlef2_supS\n",
      "wapcandlef3_supS\n",
      "wapcandlef41_supS\n",
      "wapcandlef42_supS\n",
      "midBSnum\n",
      "waproll_std5_midBS_mean\n",
      "waproll_std5_midBS_std\n",
      "waproll_std5_midBS_skew\n",
      "waproll_std5_midBS_autocorr\n",
      "waproll_std10_midBS_mean\n",
      "waproll_std10_midBS_std\n",
      "waproll_std10_midBS_skew\n",
      "waproll_std10_midBS_autocorr\n",
      "wapcandlef1_midBS\n",
      "wapcandlef2_midBS\n",
      "wapcandlef3_midBS\n",
      "wapcandlef41_midBS\n",
      "wapcandlef42_midBS\n",
      "up50num\n",
      "waproll_std5_up50_mean\n",
      "waproll_std5_up50_std\n",
      "waproll_std5_up50_skew\n",
      "waproll_std5_up50_autocorr\n",
      "waproll_std10_up50_mean\n",
      "waproll_std10_up50_std\n",
      "waproll_std10_up50_skew\n",
      "waproll_std10_up50_autocorr\n",
      "wapcandlef1_up50\n",
      "wapcandlef2_up50\n",
      "wapcandlef3_up50\n",
      "wapcandlef41_up50\n",
      "wapcandlef42_up50\n",
      "down50num\n",
      "waproll_std5_down50_mean\n",
      "waproll_std5_down50_std\n",
      "waproll_std5_down50_skew\n",
      "waproll_std5_down50_autocorr\n",
      "waproll_std10_down50_mean\n",
      "waproll_std10_down50_std\n",
      "waproll_std10_down50_skew\n",
      "waproll_std10_down50_autocorr\n",
      "wapcandlef1_down50\n",
      "wapcandlef2_down50\n",
      "wapcandlef3_down50\n",
      "wapcandlef41_down50\n",
      "wapcandlef42_down50\n",
      "up25num\n",
      "waproll_std5_up25_mean\n",
      "waproll_std5_up25_std\n",
      "waproll_std5_up25_skew\n",
      "waproll_std5_up25_autocorr\n",
      "waproll_std10_up25_mean\n",
      "waproll_std10_up25_std\n",
      "waproll_std10_up25_skew\n",
      "waproll_std10_up25_autocorr\n",
      "wapcandlef1_up25\n",
      "wapcandlef2_up25\n",
      "wapcandlef3_up25\n",
      "wapcandlef41_up25\n",
      "wapcandlef42_up25\n",
      "down25num\n",
      "waproll_std5_down25_mean\n",
      "waproll_std5_down25_std\n",
      "waproll_std5_down25_skew\n",
      "waproll_std5_down25_autocorr\n",
      "waproll_std10_down25_mean\n",
      "waproll_std10_down25_std\n",
      "waproll_std10_down25_skew\n",
      "waproll_std10_down25_autocorr\n",
      "wapcandlef1_down25\n",
      "wapcandlef2_down25\n",
      "wapcandlef3_down25\n",
      "wapcandlef41_down25\n",
      "wapcandlef42_down25\n",
      "up75num\n",
      "waproll_std5_up75_mean\n",
      "waproll_std5_up75_std\n",
      "waproll_std5_up75_skew\n",
      "waproll_std5_up75_autocorr\n",
      "waproll_std10_up75_mean\n",
      "waproll_std10_up75_std\n",
      "waproll_std10_up75_skew\n",
      "waproll_std10_up75_autocorr\n",
      "wapcandlef1_up75\n",
      "wapcandlef2_up75\n",
      "wapcandlef3_up75\n",
      "wapcandlef41_up75\n",
      "wapcandlef42_up75\n",
      "down75num\n",
      "waproll_std5_down75_mean\n",
      "waproll_std5_down75_std\n",
      "waproll_std5_down75_skew\n",
      "waproll_std5_down75_autocorr\n",
      "waproll_std10_down75_mean\n",
      "waproll_std10_down75_std\n",
      "waproll_std10_down75_skew\n",
      "waproll_std10_down75_autocorr\n",
      "wapcandlef1_down75\n",
      "wapcandlef2_down75\n",
      "wapcandlef3_down75\n",
      "wapcandlef41_down75\n",
      "wapcandlef42_down75\n",
      "stock_id\n",
      "book_wap1<lambda>\n",
      "book_wap2<lambda>\n",
      "book_wap_mean<lambda>\n",
      "book_wap_diff<lambda>\n",
      "book_price_spread<lambda>\n",
      "book_bid_spread<lambda>\n",
      "book_ask_spread<lambda>\n",
      "book_total_volume<lambda>\n",
      "book_volume_imbalance<lambda>\n",
      "pricesum\n",
      "pricemean\n",
      "pricestd\n",
      "pricemax\n",
      "pricemin\n",
      "sizesum\n",
      "sizemean\n",
      "sizestd\n",
      "sizemax\n",
      "sizemin\n",
      "order_countsum\n",
      "order_countmean\n",
      "order_countstd\n",
      "order_countmax\n",
      "order_countmin\n",
      "seconds_in_bucketsum\n",
      "seconds_in_bucketmean\n",
      "seconds_in_bucketstd\n",
      "seconds_in_bucketmax\n",
      "seconds_in_bucketmin\n"
     ]
    }
   ],
   "source": [
    "for i in temp.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}